{
  "retrieval": {
    "per_query": [
      {
        "query": "What is RAG?",
        "precision": 0.3333333333333333,
        "recall": 1.0,
        "f1": 0.5,
        "ap": 0.5,
        "rr": 0.5,
        "retrieved": [
          "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
          "https://huyenchip.com/2024/07/25/genai-platform.html",
          "https://lilianweng.github.io/posts/2024-07-07-hallucination"
        ],
        "relevant": [
          "https://huyenchip.com/2024/07/25/genai-platform.html"
        ]
      },
      {
        "query": "What is colbert?",
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "ap": 1.0,
        "rr": 1.0,
        "retrieved": [
          "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"
        ],
        "relevant": [
          "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"
        ]
      },
      {
        "query": "What Causes Hallucinations?",
        "precision": 0.5,
        "recall": 1.0,
        "f1": 0.6666666666666666,
        "ap": 1.0,
        "rr": 1.0,
        "retrieved": [
          "https://lilianweng.github.io/posts/2024-07-07-hallucination",
          "https://huyenchip.com/2024/07/25/genai-platform.html"
        ],
        "relevant": [
          "https://lilianweng.github.io/posts/2024-07-07-hallucination"
        ]
      }
    ],
    "macro_precision": 0.611111111111111,
    "macro_recall": 1.0,
    "macro_f1": 0.7222222222222222,
    "micro_precision": 0.5,
    "micro_recall": 1.0,
    "micro_f1": 0.6666666666666666,
    "MAP": 0.8333333333333334,
    "MRR": 0.8333333333333334,
    "top_k": 5
  },
  "generation": {
    "per_query": [
      {
        "query": "What is RAG?",
        "reference": "The most well-known pattern for context construction is RAG, Retrieval-Augmented Generation. RAG consists of two components: a generator (e.g. a language model) and a retriever, which retrieves relevant information from external sources.",
        "generated": "Copy\nRAGatouille is a new Python library that facilitates the use of advanced retrieval methods within RAG pipelines. It's designed for modularity and easy integration, allowing users to leverage state-of-the-art research seamlessly. The main goal of RAGatouille is to simplify the application of complex models like ColBERT in RAG pipelines, making it accessible for developers to utilize these meth Retrieval isn’t unique to RAGs. It’s the backbone of search engines, recommender systems, log analytics, etc. Many retrieval algorithms developed for traditional retrieval systems can be used for RAGs.\nExternal memory sources typically contain unstructured data, such as memos, contracts, news updates, etc. They can be collectively called documents . A document can be 10 tokens or 1 million tokens. In-context learning, learning from the context, is a form of continual learning. It enables a model to incorporate new information continually to make decisions, preventing it from becoming outdated. For example, a model trained on last-week data won’t be able to answer questions about this week unless the new information is included in its context. By updating a model’s context with the latest in Another pattern is ensemble. Remember that a retriever works by ranking documents by their relevance scores to the query. You use multiple retrievers to fetch candidates at the same time, then combine these different rankings together to generate a final ranking.\nRAGs with tabular data\nExternal data sources can also be structured, such as dataframes or SQL tables. Retrieving data from an SQL table RARR (“Retrofit Attribution using Research and Revision”; Gao et al. 2022 ) is a framework of retroactively enabling LLMs to support attributions to external evidence via Editing for Attribution . Given a model generated text $x$, RARR processes in two steps, outputting a revised text $y$ and an attribution report $A$ :\nResearch stage : Find related documents as evidence. (1) First use a query gen",
        "sources": [
          "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search",
          "https://huyenchip.com/2024/07/25/genai-platform.html",
          "https://lilianweng.github.io/posts/2024-07-07-hallucination"
        ],
        "embed_cosine": 0.6134824029778448
      },
      {
        "query": "What is colbert?",
        "reference": "The name ColBERT stands for Contextualized Late Interaction over BERT, a model stems from the Stanford University, that leverages the deep language understanding of BERT while introducing a novel interaction mechanism. This mechanism, known as late interaction, allows for efficient and precise retrieval by processing queries and documents separately until the final stages of the retrieval process. Specifically, there are two versions of the model: ColBERT: The initial model was the brainchild of Omar Khattab and Matei Zaharia, presenting a novel approach to information retrieval through the ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT paper. Their work was published in SIGIR 2020.",
        "generated": "The original ColBERT paper that introduces the \"late interaction\".\nColBERTv2 : Building on the foundational work, Omar Khattab continued his research, collaborating with Barlas Oguz, Matei Zaharia, and Michael S. Bernstein to introduce \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,\" presented at SIGIR 2021. This next iteration of ColBERT addressed previous limitati .\nColBERTv2 adding denoised supervision and residual compression to improve the training data's quality and reduce the space footprint.\nUnderstand ColBERT's Design\nGiven that ColBERTv2's architecture remains very similar to that of the original ColBERT, with its key innovations revolving around training techniques and compression mechanisms, we will first delve into the foundational aspects of the ColBERT : The initial model was the brainchild of Omar Khattab and Matei Zaharia , presenting a novel approach to information retrieval through the \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\" paper. Their work was published in SIGIR 2020.\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT Last Friday, the release of the ColBERT model by Jina AI on Hugging Face sparked significant excitement across the AI community, particularly on Twitter/X. While many are familiar with the groundbreaking BERT model, the buzz around ColBERT has left some wondering: What makes ColBERT stand out in the crowded field of information retrieval technologies? Why the AI community is excited about 8192-len Practicality for Real-World Applications : The experimental results underline ColBERT's practical applicability for real-world scenarios. Its indexing throughput and memory efficiency make it suitable for indexing large document collections like MS MARCO within a few hours, retaining high effectiveness with a manageable space footprint. These qualities highlight ColBERT's suitability for deploymen",
        "sources": [
          "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search"
        ],
        "embed_cosine": 0.8628075833048255
      },
      {
        "query": "What Causes Hallucinations?",
        "reference": "In-context hallucination: The model output should be consistent with the source content in context. Extrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so. This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.",
        "generated": "Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world In-context hallucination: The model output should be consistent with the source content in context.\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essenti [14] Mishra et al. “Fine-grained Hallucination Detection and Editing for Language Models.” arXiv preprint arXiv:2401.06855 (2024).\n[15] Lee, et al. “Factuality Enhanced Language Models for Open-Ended Text Generation.” NeuriPS 2022.\n[16] Manakul et al. “SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.” EMNLP 2023.\n[17] Li et al. “Inference-Time Int Indirect Query #\nAgrawal et al. (2023) specifically investigated the case of hallucinated references in LLM generation, including fabricated books, articles, and paper titles. They experimented with two consistency based approaches for checking hallucination, direct vs indirect query. Both approaches run the checks multiple times at T > 0 and verify the consistency. Factual inconsistent responses hallucinated by the model. Hallucination detection is an active area of research with solutions such as SelfCheckGPT (Manakul et al., 2023) and SAFE , Search Engine Factuality Evaluator (Wei et al., 2024). You can mitigate hallucinations by providing models with sufficient context and prompting techniques such as chain-of-thought. Hallucination detection and mitigati",
        "sources": [
          "https://lilianweng.github.io/posts/2024-07-07-hallucination",
          "https://huyenchip.com/2024/07/25/genai-platform.html"
        ],
        "embed_cosine": 0.7554450241073443
      }
    ],
    "avg_embed_cosine": 0.7439116701300049,
    "top_k": 5
  }
}
